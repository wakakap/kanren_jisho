import streamlit as st
import sqlite3
from jamdict import Jamdict
import os
import opencc
import re
from pykakasi import kakasi

# --- 1. åˆå§‹åŒ–ä¸é…ç½® (ä¸ä¹‹å‰ç›¸åŒ) ---
APP_DIR = os.path.dirname(os.path.abspath(__file__))
JMD_XML_PATH = os.path.join(APP_DIR, 'JMdict.xml')
JMD_DB_PATH = os.path.join(APP_DIR, 'JMdict.db')
FAV_DB_PATH = os.path.join(APP_DIR, 'favorites.db')

COMMONALITY_SCORES = {
    'ichi1': 25, 'ichi2': 15, 'news1': 20, 'news2': 10,
    'gai1': 18, 'gai2': 8, 'spec1': 12, 'spec2': 5,
}
POS_SCORES = {'v': 10, 'adj': 8, 'adv': 6, 'n': 5}
RESULT_LIMIT = 30

# --- 2. èµ„æºåŠ è½½ (ä¸ä¹‹å‰ç›¸åŒ) ---
# @st.cache_resource
def get_jamdict_instance():
    if not os.path.exists(JMD_XML_PATH):
        st.error(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ° '{JMD_XML_PATH}' æ–‡ä»¶ã€‚")
        st.stop()
    try:
        return Jamdict(db_file=JMD_DB_PATH, jmd_xml_file=JMD_XML_PATH, connect_args={'check_same_thread': False})
    except Exception as e:
        st.error(f"åŠ è½½è¯å…¸æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        st.stop()

@st.cache_resource
def get_kakasi_instance():
    return kakasi()

# --- 3. æ ¸å¿ƒåŠŸèƒ½ (ä¸ä¹‹å‰ç›¸åŒ) ---
def is_romaji(text):
    return bool(re.match(r"^[a-zA-ZÅÅ«ÄÄ«Ä“]+$", text))

@st.cache_data
def convert_to_japanese_char(input_char, area='Simplified'):
    if area == 'Simplified':
        converter1 = opencc.OpenCC('s2t.json')
        converter2 = opencc.OpenCC('t2jp.json')
        return converter2.convert(converter1.convert(input_char))
    return input_char

def replace_zh_to_jp(query):
    return "".join([convert_to_japanese_char(char) for char in query])

def only_kanji(query):
    return "".join(re.findall(r'[\u4e00-\u9faf]', query))

def special_tolerant_convert(query):
    """
    (å·²æ›´æ–°) æ›´æ™ºèƒ½çš„ç‰¹æ®ŠéŸ³å˜å®¹é”™å‡½æ•°ã€‚
    1. ç§»é™¤æˆ–æ·»åŠ ä¿ƒéŸ³ `ã£`ã€‚
    2. å¯¹ç‰‡å‡åçš„é•¿éŸ³ `ãƒ¼` å®¹é”™ã€‚
    """
    variants = set()

    # --- 1. ä¿ƒéŸ³ `ã£` çš„æ’å…¥ä¸åˆ é™¤ ---

    # è§„åˆ™1: å¦‚æœå­˜åœ¨ä¿ƒéŸ³ï¼Œç”Ÿæˆä¸€ä¸ªå°†å…¶ç§»é™¤çš„ç‰ˆæœ¬
    if 'ã£' in query:
        variants.add(query.replace('ã£', ''))

    # è§„åˆ™2: åœ¨æ‰€æœ‰å‘éŸ³åˆæ³•çš„ä½ç½®å°è¯•æ’å…¥ä¿ƒéŸ³
    # å®šä¹‰å¯ä»¥æ¥åœ¨ä¿ƒéŸ³åé¢çš„å‡å (k, s, t, pè¡Œ)
    SOKUON_KANA = {
        'ã‹', 'ã', 'ã', 'ã‘', 'ã“', 'ãã‚ƒ', 'ãã‚…', 'ãã‚‡',
        'ã•', 'ã—', 'ã™', 'ã›', 'ã', 'ã—ã‚ƒ', 'ã—ã‚…', 'ã—ã‚‡',
        'ãŸ', 'ã¡', 'ã¤', 'ã¦', 'ã¨', 'ã¡ã‚ƒ', 'ã¡ã‚…', 'ã¡ã‚‡',
        'ã±', 'ã´', 'ã·', 'ãº', 'ã½', 'ã´ã‚ƒ', 'ã´ã‚…', 'ã´ã‚‡',
        'ã‚«', 'ã‚­', 'ã‚¯', 'ã‚±', 'ã‚³', 'ã‚­ãƒ£', 'ã‚­ãƒ¥', 'ã‚­ãƒ§',
        'ã‚µ', 'ã‚·', 'ã‚¹', 'ã‚»', 'ã‚½', 'ã‚·ãƒ£', 'ã‚·ãƒ¥', 'ã‚·ãƒ§',
        'ã‚¿', 'ãƒ', 'ãƒ„', 'ãƒ†', 'ãƒˆ', 'ãƒãƒ£', 'ãƒãƒ¥', 'ãƒãƒ§',
        'ãƒ‘', 'ãƒ”', 'ãƒ—', 'ãƒš', 'ãƒ', 'ãƒ”ãƒ£', 'ãƒ”ãƒ¥', 'ãƒ”ãƒ§'
    }
    for i in range(1, len(query)):
        # å¦‚æœå½“å‰ä½ç½®çš„å‡åå¯ä»¥æ¥åœ¨ä¿ƒéŸ³åï¼Œå¹¶ä¸”å®ƒå‰é¢ä¸æ˜¯ä¸€ä¸ªä¿ƒéŸ³
        if query[i] in SOKUON_KANA and query[i-1] != 'ã£':
            # ç”Ÿæˆæ’å…¥ä¿ƒéŸ³åçš„æ–°è¯
            new_variant = query[:i] + 'ã£' + query[i:]
            variants.add(new_variant)
            
    return list(variants)

def get_commonality_score(entry):
    all_priorities = set()
    for form in entry.kanji_forms: all_priorities.update(form.pri)
    for form in entry.kana_forms: all_priorities.update(form.pri)
    return sum(COMMONALITY_SCORES.get(p, 0) for p in all_priorities)

def get_pos_score(entry):
    max_score = 0
    for sense in entry.senses:
        for pos in sense.pos:
            score = POS_SCORES.get(pos.split('-')[0], 1)
            if score > max_score: max_score = score
    return max_score

# --- 4. æ•°æ®åº“ä¸UIè¾…åŠ©å‡½æ•° (display_entries æœ‰å°è°ƒæ•´) ---
def add_to_favorites(entry):
    word = entry.kanji_forms[0].text if entry.kanji_forms else entry.kana_forms[0].text
    reading = entry.kana_forms[0].text if entry.kana_forms else ""
    definition = "; ".join([f"{i+1}. {s.text()}" for i, s in enumerate(entry.senses)])
    
    conn = sqlite3.connect(FAV_DB_PATH, check_same_thread=False)
    try:
        cursor = conn.cursor()
        cursor.execute("INSERT INTO favorites (word, reading, definition) VALUES (?, ?, ?)", (word, reading, definition))
        conn.commit()
        st.toast(f"'{word}' å·²æ·»åŠ åˆ°æ”¶è—å¤¹ï¼")
        st.rerun()
    except sqlite3.IntegrityError:
        st.toast(f"'{word}' å·²åœ¨æ”¶è—å¤¹ä¸­ã€‚")
    except Exception as e:
        st.error(f"æ·»åŠ å¤±è´¥: {e}")
    finally:
        conn.close()

def get_favorites():
    conn = sqlite3.connect(FAV_DB_PATH, check_same_thread=False)
    try:
        cursor = conn.cursor()
        cursor.execute("SELECT word, reading, definition FROM favorites ORDER BY id DESC")
        return cursor.fetchall()
    finally:
        conn.close()

def remove_from_favorites(word, definition):
    conn = sqlite3.connect(FAV_DB_PATH, check_same_thread=False)
    try:
        cursor = conn.cursor()
        cursor.execute("DELETE FROM favorites WHERE word = ? AND definition = ?", (word, definition))
        conn.commit()
        st.toast(f"'{word}' å·²ä»æ”¶è—å¤¹ç§»é™¤ã€‚")
        st.rerun()
    except Exception as e:
        st.error(f"ç§»é™¤å¤±è´¥: {e}")
    finally:
        conn.close()
def set_search_query(query):
    """(æ–°å¢) ç”¨äºå»ºè®®è¯æŒ‰é’®çš„å›è°ƒå‡½æ•°ï¼Œè®¾ç½®æ–°çš„æœç´¢è¯"""
    st.session_state.next_search_query = query

def display_suggestions(entries):
    """(æ–°å¢) åœ¨æŒ‡å®šå®¹å™¨ä¸­æ¨ªå‘æ¸²æŸ“å»ºè®®è¯"""
    st.markdown("---")
    st.write("æ‚¨æ˜¯ä¸æ˜¯æƒ³æ‰¾ï¼š")
    
    # æ¯è¡Œæœ€å¤šæ˜¾ç¤º5ä¸ªå»ºè®®è¯
    cols = st.columns(5)
    col_idx = 0
    for entry in entries:
        word = entry.kanji_forms[0].text if entry.kanji_forms else entry.kana_forms[0].text
        reading = entry.kana_forms[0].text if entry.kana_forms else ""
        
        # ä½¿ç”¨å›è°ƒå‡½æ•°æ¥æ›´æ–°æœç´¢æ¡†å†…å®¹
        cols[col_idx].button(
            label=f"{word} `{reading}`", 
            key=f"sug_{entry.idseq}",
            on_click=set_search_query,
            args=(word,) # å°†è¯è¯­æœ¬èº«ä½œä¸ºå‚æ•°ä¼ é€’ç»™å›è°ƒ
        )
        col_idx = (col_idx + 1) % 5

def find_sokuon_suggestions(jmd, query, exclude_ids):
    """(æ–°å¢) æŸ¥æ‰¾ä¿ƒéŸ³å®¹é”™çš„å»ºè®®è¯"""
    suggestion_entries = []
    found_sug_ids = set()

    # ç”Ÿæˆä¿ƒéŸ³å®¹é”™çš„å˜ä½“
    variants = special_tolerant_convert(query)
    
    for variant in variants:
        # å»ºè®®è¯ä¸éœ€è¦å¤ªå¤šï¼Œé™åˆ¶ä¸€ä¸‹æ•°é‡
        if len(suggestion_entries) >= 5:
            break
        
        # ç²¾ç¡®åŒ¹é…è¿™äº›å˜ä½“
        lookup_result = jmd.lookup(variant)
        for entry in lookup_result.entries:
            # ç¡®ä¿ä¸ä¸ä¸»ç»“æœé‡å¤ï¼Œå¹¶ä¸”å»ºè®®ç»“æœè‡ªèº«ä¸é‡å¤
            if entry.idseq not in exclude_ids and entry.idseq not in found_sug_ids:
                suggestion_entries.append(entry)
                found_sug_ids.add(entry.idseq)
                if len(suggestion_entries) >= 5:
                    break
    
    return suggestion_entries

def display_entries(entries):
    """(å·²ä¿®æ­£) åœ¨å½“å‰ç¯å¢ƒä¸­ç»˜åˆ¶è¯æ¡åˆ—è¡¨"""
    # with container: è¢«ç§»é™¤
    for entry in entries:
        word_display = entry.kanji_forms[0].text if entry.kanji_forms else entry.kana_forms[0].text
        reading_display = entry.kana_forms[0].text if entry.kana_forms else ""
        
        with st.container(border=True):
            res_col1, res_col2 = st.columns([4, 1])
            with res_col1:
                st.subheader(f"{word_display} `{reading_display}`")
                for i, sense in enumerate(entry.senses):
                    st.markdown(f"**{i+1}.** {sense.text()}")
            with res_col2:
                if st.button("â­ æ”¶è—", key=f"add_{entry.idseq}"):
                    add_to_favorites(entry)


# --- 5. Streamlit ç”¨æˆ·ç•Œé¢ (æ ¸å¿ƒä¿®æ”¹åŒºåŸŸ) ---
st.set_page_config(page_title="æˆ‘çš„æ™ºèƒ½æ—¥è¯­è¯å…¸", layout="wide")

jmd = get_jamdict_instance()
kks = get_kakasi_instance()

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'search_status' not in st.session_state:
    st.session_state.search_status = 'INIT'
    st.session_state.search_query_input = ""
    st.session_state.search_query = ""
    st.session_state.processed_query = ""
    st.session_state.tier1_entries = []
    st.session_state.sokuon_suggestions = []
    st.session_state.tier2_entries = []
    st.session_state.tier3_entries = []
    st.session_state.found_ids = set()
    st.session_state.debug_log = []
    
# è¿™ä¸ªé€»è¾‘å¿…é¡»åœ¨æ‰€æœ‰UIç»„ä»¶ï¼ˆå°¤å…¶æ˜¯st.text_inputï¼‰è¢«åˆ›å»ºä¹‹å‰è¿è¡Œ
if 'next_search_query' in st.session_state:
    st.session_state.search_query_input = st.session_state.next_search_query
    del st.session_state.next_search_query

# --- ä¾§è¾¹æ  (ä¿æŒä¸å˜) ---
with st.sidebar:
    st.title("â­ æ”¶è—å¤¹")
    favorites = get_favorites()
    if not favorites: st.info("è¿™é‡Œè¿˜æ²¡æœ‰æ”¶è—çš„å•è¯ã€‚")
    for fav in favorites:
        word, reading, definition = fav
        with st.container(border=True):
            st.markdown(f"**{word}** `{reading}`")
            st.caption(definition.replace("; ", "\n- "))
            if st.button("ç§»é™¤", key=f"del_{word}_{definition}"):
                remove_from_favorites(word, definition)

# --- ä¸»ç•Œé¢ ---
st.title("ğŸ“– æˆ‘çš„æ™ºèƒ½æ—¥è¯­è¯å…¸")
st.markdown("æ”¯æŒç®€/ç¹ä½“ä¸­æ–‡ã€å‡åã€ç½—é©¬éŸ³è¾“å…¥ï¼Œå¹¶é‡‡ç”¨æ™ºèƒ½åˆ†å±‚æœç´¢ä¸æ’åºã€‚")

col_main, col_debug = st.columns([2, 1])

with col_main:
    # ç»‘å®š text_input çš„å€¼ä¸º session_state.search_query_input
    search_query = st.text_input("è¾“å…¥æ—¥è¯­ã€å‡åã€ç½—é©¬éŸ³æˆ–ç®€/ç¹ä½“æ±‰å­—è¿›è¡Œæœç´¢ï¼š", 
                                 key="search_query_input", # ä½¿ç”¨keyæ¥ç»‘å®š
                                 help="ä¾‹å¦‚: taberu, é£Ÿã¹ã‚‹, ãŒã£ã“ã†, å­¦æ ¡")
    
    st.markdown("---")
    tier1_placeholder = st.empty()
    suggestion_placeholder = st.empty() # <--- æ–°å¢å»ºè®®è¯çš„å ä½ç¬¦
    tier2_placeholder = st.empty()
    tier3_placeholder = st.empty()
    no_results_placeholder = st.empty()


with col_debug:
    st.markdown("### âš™ï¸ æœç´¢è¿‡ç¨‹åˆ†æ")
    debug_placeholder = st.empty()

# --- ä¸»è¦æœç´¢é€»è¾‘ ---

# å½“ç”¨æˆ·è¾“å…¥æ–°çš„æœç´¢è¯æ—¶ï¼Œè¿›è¡ŒéªŒè¯å¹¶å‡†å¤‡é‡ç½®çŠ¶æ€æœº
if search_query and search_query != st.session_state.search_query:
    
    # --- æ–°å¢çš„éªŒè¯é€»è¾‘ ---
    # åˆ¤æ–­è¾“å…¥æ˜¯å¦ä¸ºå•ä¸ªéæ±‰å­—å­—ç¬¦
    is_single_non_kanji = len(search_query) == 1 and not re.match(r'[\u4e00-\u9faf]', search_query)

    if is_single_non_kanji:
        # å¦‚æœæ˜¯æ— æ•ˆçš„çŸ­æŸ¥è¯¢ï¼Œåˆ™ä¸å¯åŠ¨æœç´¢ï¼Œåªæ›´æ–°çŠ¶æ€å¹¶æ˜¾ç¤ºæç¤º
        st.session_state.search_query = search_query # æ›´æ–°æŸ¥è¯¢è¯ä»¥é˜²æ­¢é‡å¤æç¤º
        st.session_state.search_status = 'INVALID_INPUT' # è®¾ç½®ä¸€ä¸ªæ–°çŠ¶æ€
        # æ¸…ç©ºä¸Šä¸€è½®çš„ç»“æœ
        st.session_state.tier1_entries = []
        st.session_state.tier2_entries = []
        st.session_state.tier3_entries = []
        st.session_state.found_ids = set()
        st.session_state.debug_log = ["ä¸ºæé«˜æ•ˆç‡ï¼Œè¯·è¾“å…¥ä¸€ä¸ªä»¥ä¸Šçš„å‡å/å­—æ¯ï¼Œæˆ–ä¸€ä¸ªæ±‰å­—ã€‚"]

    else:
        # å¦‚æœæ˜¯æœ‰æ•ˆæŸ¥è¯¢ï¼Œåˆ™æŒ‰åŸè®¡åˆ’å¯åŠ¨æœç´¢çŠ¶æ€æœº
        st.session_state.search_status = 'SEARCHING_TIER_1'
        st.session_state.search_query = search_query
        # æ¸…ç©ºä¸Šä¸€è½®çš„ç»“æœ
        st.session_state.tier1_entries = []
        st.session_state.tier2_entries = []
        st.session_state.tier3_entries = []
        st.session_state.found_ids = set()
        st.session_state.debug_log = []
        # ç«‹å³é‡è·‘ä»¥å¯åŠ¨æœç´¢æµç¨‹çš„ç¬¬ä¸€æ­¥
        st.rerun()

# --- æ¸²æŸ“é€»è¾‘ (æ— è®ºå¤„äºå“ªä¸ªçŠ¶æ€ï¼Œéƒ½å…ˆæ ¹æ®å½“å‰ session_state çš„å†…å®¹æ¸²æŸ“) ---
# è¿™ä¸ªæ¨¡å—è¢«ç§»åŠ¨åˆ°äº†è®¡ç®—é€»è¾‘ä¹‹å‰
if st.session_state.search_query:
    # æ¸²æŸ“æ—¥å¿—
    debug_placeholder.markdown("\n".join(st.session_state.debug_log))

    # æ¸²æŸ“ Tier 1 ç»“æœ
    if st.session_state.tier1_entries:
        with tier1_placeholder.container():
            st.subheader("ç²¾ç¡®åŒ¹é…ç»“æœ")
            display_entries(st.session_state.tier1_entries)

    # --- æ–°å¢ï¼šæ¸²æŸ“å»ºè®®è¯ ---
    if st.session_state.sokuon_suggestions:
        with suggestion_placeholder.container():
            display_suggestions(st.session_state.sokuon_suggestions)
    
    # æ¸²æŸ“ Tier 2 ç»“æœ
    if st.session_state.tier2_entries:
        with tier2_placeholder.container():
            st.subheader("å‰ç¼€åŒ¹é…ç»“æœ")
            display_entries(st.session_state.tier2_entries)
            
    # æ¸²æŸ“ Tier 3 ç»“æœ
    if st.session_state.tier3_entries:
        with tier3_placeholder.container():
            st.subheader("å®¹é”™åŒ¹é…ç»“æœ")
            display_entries(st.session_state.tier3_entries)

    # å¦‚æœæœç´¢å®Œæˆä¸”æ²¡æœ‰ä»»ä½•ç»“æœï¼Œæ˜¾ç¤ºæç¤º
    if st.session_state.search_status == 'DONE' and not st.session_state.found_ids:
        no_results_placeholder.warning(f"æ‰¾ä¸åˆ°ä¸ '{st.session_state.search_query}' ç›¸å…³çš„ç»“æœã€‚è¯·å°è¯•å…¶ä»–å…³é”®è¯ã€‚")
else:
    debug_placeholder.info("è¾“å…¥å…³é”®è¯åï¼Œè¿™é‡Œä¼šæ˜¾ç¤ºæœç´¢å’Œæ’åºçš„è¯¦ç»†æ­¥éª¤ã€‚")


# --- çŠ¶æ€æœºé©±åŠ¨çš„è®¡ç®—é€»è¾‘ (åœ¨æ¸²æŸ“é€»è¾‘ä¹‹åæ‰§è¡Œ) ---
try:
    # çŠ¶æ€1: æ­£åœ¨æœç´¢ Tier 1
    if st.session_state.search_status == 'SEARCHING_TIER_1':
        debug_log = st.session_state.debug_log
        
        # 0. é¢„å¤„ç† (åªåœ¨ç¬¬ä¸€æ­¥æ‰§è¡Œ)
        debug_log.append(f"**åŸå§‹è¾“å…¥:** `{st.session_state.search_query}`")
        if is_romaji(st.session_state.search_query):
            processed_query = kks.convert(st.session_state.search_query)[0]['hira']
            debug_log.append(f"**ç±»å‹åˆ¤æ–­:** ç½—é©¬éŸ³ -> `{processed_query}`")
        else:
            processed_query = replace_zh_to_jp(st.session_state.search_query)
            if processed_query != st.session_state.search_query: debug_log.append(f"**ç±»å‹åˆ¤æ–­:** ä¸­æ–‡ -> `{processed_query}`")
            else: debug_log.append(f"**ç±»å‹åˆ¤æ–­:** æ—¥æ–‡")
        st.session_state.processed_query = processed_query
        
        # Tier 1: å®Œå…¨åŒ¹é…
        debug_log.append("\n---\n**å±‚çº§ 1: å®Œå…¨åŒ¹é…**\n---")
        lookup_result = jmd.lookup(st.session_state.processed_query)
        for entry in lookup_result.entries:
            if entry.idseq not in st.session_state.found_ids:
                st.session_state.tier1_entries.append(entry)
                st.session_state.found_ids.add(entry.idseq)
        debug_log.append(f"æ‰¾åˆ° {len(st.session_state.tier1_entries)} ä¸ªæ–°ç»“æœã€‚")
        # --- æ–°å¢ï¼šåœ¨è¿™é‡ŒæŸ¥æ‰¾å»ºè®®è¯ ---
        debug_log.append("\n---\n**å»ºè®®è¯: æŸ¥æ‰¾ä¿ƒéŸ³å®¹é”™**\n---")
        # æŸ¥æ‰¾å»ºè®®è¯ï¼Œå¹¶ç¡®ä¿å®ƒä»¬ä¸å’Œå·²æ‰¾åˆ°çš„ç²¾ç¡®åŒ¹é…ç»“æœé‡å¤
        suggestions = find_sokuon_suggestions(jmd, st.session_state.processed_query, st.session_state.found_ids)
        st.session_state.sokuon_suggestions = suggestions
        debug_log.append(f"æ‰¾åˆ° {len(suggestions)} ä¸ªå»ºè®®è¯ã€‚")
        # --- å»ºè®®è¯æŸ¥æ‰¾ç»“æŸ ---
        
        st.session_state.search_status = 'SEARCHING_TIER_2'
        st.rerun()

    # çŠ¶æ€2: æ­£åœ¨æœç´¢ Tier 2
    elif st.session_state.search_status == 'SEARCHING_TIER_2':
        debug_log = st.session_state.debug_log
        processed_query = st.session_state.processed_query
        
        debug_log.append("\n---\n**å±‚çº§ 2: å‰ç¼€åŒ¹é…**\n---")
        lookup_result = jmd.lookup(f"{processed_query}%")
        for entry in lookup_result.entries:
            if entry.idseq not in st.session_state.found_ids:
                st.session_state.tier2_entries.append(entry)
                st.session_state.found_ids.add(entry.idseq)
        debug_log.append(f"æ‰¾åˆ° {len(st.session_state.tier2_entries)} ä¸ªæ–°ç»“æœã€‚")

        if not st.session_state.found_ids:
            st.session_state.search_status = 'SEARCHING_TIER_3'
        else:
            st.session_state.search_status = 'DONE'
            debug_log.append("\n---\n**æ‰€æœ‰æœç´¢å·²å®Œæˆ**\n---")
        st.rerun()

    # çŠ¶æ€3: æ­£åœ¨æœç´¢ Tier 3
    elif st.session_state.search_status == 'SEARCHING_TIER_3':
        debug_log = st.session_state.debug_log
        processed_query = st.session_state.processed_query
        
        debug_log.append("\n---\n**å±‚çº§ 3: å®¹é”™åŒ¹é…**\n---")
        tolerant_queries = set()
        for variant in special_tolerant_convert(processed_query): tolerant_queries.add(variant)
        if len(processed_query) > 2: tolerant_queries.add(processed_query[:-1])
        kanji_only_str = only_kanji(processed_query)
        if kanji_only_str and kanji_only_str != processed_query: tolerant_queries.add(kanji_only_str)
        debug_log.append(f"ç”Ÿæˆå®¹é”™æœç´¢è¯: `{list(tolerant_queries)}`")
        
        for t_query in tolerant_queries:
            if not t_query: continue
            lookup_result = jmd.lookup(f"{t_query}%")
            for entry in lookup_result.entries:
                if entry.idseq not in st.session_state.found_ids:
                    st.session_state.tier3_entries.append(entry)
                    st.session_state.found_ids.add(entry.idseq)
        debug_log.append(f"æ‰¾åˆ° {len(st.session_state.tier3_entries)} ä¸ªæ–°ç»“æœã€‚")

        st.session_state.search_status = 'DONE'
        debug_log.append("\n---\n**æ‰€æœ‰æœç´¢å·²å®Œæˆ**\n---")
        st.rerun()
except Exception as e:
    # å‘ç”Ÿä»»ä½•æ„å¤–æ—¶ï¼Œå°†çŠ¶æ€é‡ç½®ï¼Œé¿å…å¡åœ¨æœç´¢ä¸­
    st.session_state.search_status = 'DONE'
    st.error(f"æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")